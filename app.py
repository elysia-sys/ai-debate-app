import streamlit as st
from google import genai
from google.genai import types
import time

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(page_title="AIãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ Pro", page_icon="ğŸ“", layout="wide")

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šè¨­å®šã‚¨ãƒªã‚¢ ---
with st.sidebar:
    st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ è¨­å®š")
    
    # 1. APIã‚­ãƒ¼å…¥åŠ›
    user_api_key = st.text_input(
        "Google API Keyã‚’å…¥åŠ›",
        type="password",
        help="APIã‚­ãƒ¼ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ãŒè‡ªå‹•ã§èª­ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚"
    )
    
    st.divider()
    
    # 2. ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆAPIã‚­ãƒ¼ã‹ã‚‰å‹•çš„ã«å–å¾—ï¼‰
    model_options = ["gemini-2.0-flash", "gemini-1.5-flash"] # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆã‚­ãƒ¼ãŒãªã„å ´åˆï¼‰
    
    if user_api_key:
        try:
            # ä¸€æ™‚çš„ã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œã£ã¦ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—
            temp_client = genai.Client(api_key=user_api_key)
            fetched_models = []
            # APIã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
            for m in temp_client.models.list():
                # "generateContent" ã«å¯¾å¿œã—ã€ã‹ã¤ "gemini" ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ«ã ã‘æŠ½å‡º
                if "generateContent" in m.supported_generation_methods and "gemini" in m.name:
                    # "models/" ã¨ã„ã†æ¥é ­è¾ã‚’å‰Šé™¤ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                    clean_name = m.name.replace("models/", "")
                    fetched_models.append(clean_name)
            
            if fetched_models:
                model_options = sorted(fetched_models, reverse=True) # æ–°ã—ã„é †ã«ä¸¦ã¹ã‚‹
                st.success("âœ… ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã—ã¾ã—ãŸ")
            
        except Exception:
            st.warning("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

    model_name = st.selectbox("ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«", model_options)
    
    # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€å¤§æ•°ã‚’50ã«å¢—åŠ ï¼‰
    max_turns = st.slider("ä¼šè©±ã®å¾€å¾©å›æ•°ï¼ˆã‚¿ãƒ¼ãƒ³æ•°ï¼‰", min_value=3, max_value=50, value=6)
    speed = st.slider("è¡¨ç¤ºé€Ÿåº¦ï¼ˆå¾…æ©Ÿç§’æ•°ï¼‰", 0.5, 5.0, 1.5)

# --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
st.title("ğŸ“ AIãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ Pro")
st.markdown("è­°è«–ã®è¨­å®šã‚’è¡Œã†ã¨ã€AIãŒä¼šè©±ã‚’è¡Œã„ã€æœ€å¾Œã«**è¦ç´„ã¨çµè«–**ã‚’ã¾ã¨ã‚ã¾ã™ã€‚")

# ãƒ†ãƒ¼ãƒè¨­å®š
topic = st.text_input("ğŸ—£ï¸ è­°è«–ãƒ»ä¼šè©±ã®ãƒ†ãƒ¼ãƒ", value="")

# ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š
st.subheader("ğŸ‘¥ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®š")
num_agents = st.number_input("å‚åŠ äººæ•°", min_value=2, max_value=4, value=2)

agents_config = []
cols = st.columns(num_agents)

default_roles = [
    {"name": "è‚¯å®šæ´¾", "icon": "â­•", "prompt": "ã‚ãªãŸã¯è‚¯å®šçš„ãªç«‹å ´ã§ã™ã€‚ãƒ¡ãƒªãƒƒãƒˆã‚’å¼·èª¿ã—ã€æœªæ¥å¿—å‘ã§è­°è«–ã—ã¦ãã ã•ã„ã€‚"},
    {"name": "å¦å®šæ´¾", "icon": "âŒ", "prompt": "ã‚ãªãŸã¯æ‰¹åˆ¤çš„ãªç«‹å ´ã§ã™ã€‚ãƒªã‚¹ã‚¯ã‚„æ‡¸å¿µç‚¹ã‚’æŒ‡æ‘˜ã—ã€æ…é‡ãªè­°è«–ã‚’æ±‚ã‚ã¦ãã ã•ã„ã€‚"},
    {"name": "ãƒ¢ãƒ‡ãƒ¬ãƒ¼ã‚¿ãƒ¼", "icon": "âš–ï¸", "prompt": "ã‚ãªãŸã¯å…¬å¹³ãªå¸ä¼šè€…ã§ã™ã€‚è­°è«–ã‚’æ•´ç†ã—ã€ä¸¡è€…ã®æ„è¦‹ã‚’å¼•ãå‡ºã—ã¦ãã ã•ã„ã€‚"},
    {"name": "è‡ªç”±äºº", "icon": "ğŸ¦„", "prompt": "ã‚ãªãŸã¯ç‹¬è‡ªã®è¦–ç‚¹ã‚’æŒã¤è‡ªç”±äººã§ã™ã€‚è­°è«–ã®æ ã«ã¨ã‚‰ã‚ã‚Œãªã„ç™ºæƒ³ã‚’å‡ºã—ã¦ãã ã•ã„ã€‚"}
]

for i, col in enumerate(cols):
    with col:
        st.markdown(f"**å‚åŠ è€… {i+1}**")
        def_role = default_roles[i] if i < len(default_roles) else default_roles[0]
        
        name = st.text_input(f"åå‰", value=def_role["name"], key=f"name_{i}")
        icon = st.text_input(f"ã‚¢ã‚¤ã‚³ãƒ³", value=def_role["icon"], key=f"icon_{i}")
        prompt = st.text_area(f"å½¹å‰²è¨­å®š", value=def_role["prompt"], height=100, key=f"prompt_{i}")
        
        agents_config.append({"name": name, "icon": icon, "system_instruction": prompt})

# --- å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ ---
if st.button("ğŸš€ è­°è«–ã‚’é–‹å§‹ã™ã‚‹", type="primary"):
    if not user_api_key:
        st.error("APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼")
        st.stop()
    
    # å…¨ä½“ã®å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆï¼ˆè¦ç´„ç”¨ï¼‰
    full_conversation_log = []
    
    try:
        client = genai.Client(api_key=user_api_key)
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æº–å‚™
        chats = []
        for agent in agents_config:
            sys_inst = f"åå‰ï¼š{agent['name']}\nå½¹å‰²ï¼š{agent['system_instruction']}\nãƒ†ãƒ¼ãƒï¼š{topic}\nä»–ã®å‚åŠ è€…ã¨è­°è«–ã—ã¦ãã ã•ã„ã€‚"
            chats.append(client.chats.create(model=model_name, config={"system_instruction": sys_inst}))

        chat_container = st.container()
        last_message = f"ãƒ†ãƒ¼ãƒã€Œ{topic}ã€ã«ã¤ã„ã¦è­°è«–ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚ã¾ãšã¯{agents_config[0]['name']}ã•ã‚“ã‹ã‚‰ã©ã†ãã€‚"
        
        # === è­°è«–ãƒ«ãƒ¼ãƒ— ===
        count = 0
        progress_bar = st.progress(0)
        
        while count < max_turns:
            current_idx = count % num_agents
            agent = agents_config[current_idx]
            chat = chats[current_idx]
            
            with chat_container:
                with st.chat_message(agent["name"], avatar=agent["icon"]):
                    placeholder = st.empty()
                    try:
                        # ç™ºè¨€ç”Ÿæˆ
                        response = chat.send_message(f"ç›´å‰ã®ç™ºè¨€: {last_message}\n\nã“ã‚Œã‚’å—ã‘ã¦ç™ºè¨€ã—ã¦ãã ã•ã„ã€‚")
                        placeholder.markdown(response.text)
                        
                        # ãƒ­ã‚°ä¿å­˜
                        last_message = f"{agent['name']}: {response.text}"
                        full_conversation_log.append(f"ã€{agent['name']}ã€‘\n{response.text}")
                        
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                        break
            
            count += 1
            progress_bar.progress(count / max_turns)
            time.sleep(speed)
        
        # === æœ€çµ‚è¦ç´„ãƒ•ã‚§ãƒ¼ã‚º ===
        st.divider()
        st.subheader("ğŸ“Š è­°è«–ã®ã¾ã¨ã‚ã¨çµè«–")
        
        with st.status("ğŸ“ AIãŒè­°äº‹éŒ²ã‚’ä½œæˆä¸­...", expanded=True) as status:
            try:
                # ãƒ­ã‚°ã‚’ä¸€ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã«çµåˆ
                log_text = "\n\n".join(full_conversation_log)
                
                # è¦ç´„ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                summary_prompt = f"""
                ã‚ãªãŸã¯å„ªç§€ãªæ›¸è¨˜å®˜ã§ã™ã€‚ä»¥ä¸‹ã®è­°è«–ãƒ­ã‚°ã‚’èª­ã‚“ã§ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

                ## è­°è«–ãƒ­ã‚°
                {log_text}

                ## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
                1. **è­°è«–ã®ãƒ†ãƒ¼ãƒ**: {topic}
                2. **å„å‚åŠ è€…ã®ä¸»ãªä¸»å¼µ**: (ç®‡æ¡æ›¸ãã§ç°¡æ½”ã«)
                3. **è­°è«–ã®è¦ç´„**: (å¯¾è©±ã®æµã‚Œã‚’è¦ç´„)
                4. **æœ€çµ‚çµè«–**: (è­°è«–ã‹ã‚‰å°ãå‡ºã•ã‚Œã‚‹çµè«–ã€ã¾ãŸã¯åˆæ„ç‚¹ã€æ®‹ã•ã‚ŒãŸèª²é¡Œ)
                """
                
                # è¦ç´„ç”Ÿæˆï¼ˆæ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ã‚ãšã€å˜ç™ºã§ç”Ÿæˆï¼‰
                summary_response = client.models.generate_content(
                    model=model_name,
                    contents=summary_prompt
                )
                
                st.markdown(summary_response.text)
                status.update(label="âœ… ä½œæˆå®Œäº†ï¼", state="complete", expanded=True)
                
            except Exception as e:
                st.error(f"è¦ç´„ã®ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

    except Exception as e:
        st.error(f"é–‹å§‹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚APIã‚­ãƒ¼ã‚„ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {e}")